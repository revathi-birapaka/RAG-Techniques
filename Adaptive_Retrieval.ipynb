{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78fd01c-15c0-447d-854a-eefd43dce697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import Dict, Any\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "load_dotenv()\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "from helper_functions import *\n",
    "from evaluation.rag_evaluation import *\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35daa4-b388-4df3-870f-0087e44b03bd",
   "metadata": {},
   "source": [
    "##### Query Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aebf727-a031-4bee-a7c3-2af149861b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class categories_options(BaseModel):\n",
    "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
    "\n",
    "\n",
    "class QueryClassifier:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
    "\n",
    "\n",
    "    def classify(self, query):\n",
    "        print(\"classifying query\")\n",
    "        return self.chain.invoke(query).category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687db906-5138-44b7-b10d-a82d93637a0f",
   "metadata": {},
   "source": [
    "##### Base Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7825ecb0-4d5c-4760-9e55-8862e428716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "\n",
    "    def retrieve(self, query, k=4):\n",
    "        return self.db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e2563-2923-4290-9cc8-6b985f9e4077",
   "metadata": {},
   "source": [
    "##### Factual Retriever Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a183e0ff-f63b-488e-9dbe-f289b20891e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relevant_score(BaseModel):\n",
    "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
    "\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving factual\")\n",
    "        # Use LLM to enhance the query\n",
    "        enhanced_query_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        )\n",
    "        query_chain = enhanced_query_prompt | self.llm\n",
    "        enhanced_query = query_chain.invoke(query).content\n",
    "        print(f'enhande query: {enhanced_query}')\n",
    "\n",
    "        # Retrieve documents using the enhanced query\n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # Use LLM to rank the relevance of retrieved documents\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"doc\"],\n",
    "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "\n",
    "        ranked_docs = []\n",
    "        print(\"ranking docs\")\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Sort by relevance score and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7b254-396f-49b3-baec-ab2af673ea50",
   "metadata": {},
   "source": [
    "##### Analytical Retriever Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beb0d24e-e486-42d9-8ea3-1c55c099c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectedIndices(BaseModel):\n",
    "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
    "\n",
    "class SubQueries(BaseModel):\n",
    "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
    "\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving analytical\")\n",
    "        # Use LLM to generate sub-queries for comprehensive analysis\n",
    "        sub_queries_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Generate {k} sub-questions for: {query}\"\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
    "\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
    "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
    "\n",
    "        all_docs = []\n",
    "        for sub_query in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
    "\n",
    "        # Use LLM to ensure diversity and relevance\n",
    "        diversity_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
    "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
    "        )\n",
    "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "        selected_indices_result = diversity_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "        \n",
    "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ab095-160c-4ee1-b056-79df88c24cbc",
   "metadata": {},
   "source": [
    "##### Opinion Retriever Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "151b1bc9-706a-4710-8be5-67476e635bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=3):\n",
    "        print(\"retrieving opinion\")\n",
    "        # Use LLM to identify potential viewpoints\n",
    "        viewpoints_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        )\n",
    "        viewpoints_chain = viewpoints_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n')\n",
    "        print(f'viewpoints: {viewpoints}')\n",
    "\n",
    "        all_docs = []\n",
    "        for viewpoint in viewpoints:\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
    "\n",
    "        # Use LLM to classify and select diverse opinions\n",
    "        opinion_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
    "        )\n",
    "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "        \n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "        selected_indices = opinion_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "        \n",
    "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a7f0f-4df3-4d52-9ed3-c0e7ec8f7780",
   "metadata": {},
   "source": [
    "##### Contextual Retriever Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51200cc0-2346-4402-828e-15ebd9fb36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4, user_context=None):\n",
    "        print(\"retrieving contextual\")\n",
    "        # Use LLM to incorporate user context into the query\n",
    "        context_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
    "        )\n",
    "        context_chain = context_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
    "        contextualized_query = context_chain.invoke(input_data).content\n",
    "        print(f'contextualized query: {contextualized_query}')\n",
    "\n",
    "        # Retrieve documents using the contextualized query\n",
    "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
    "\n",
    "        # Use LLM to rank the relevance of retrieved documents considering the user context\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\", \"doc\"],\n",
    "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "        print(\"ranking docs\")\n",
    "\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "\n",
    "        # Sort by relevance score and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14a2d8-44ed-40d5-9929-c297dc1e890d",
   "metadata": {},
   "source": [
    "##### Adaptive Retriever Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f0eb27-db4f-4ffc-93c0-9bc8cdb0854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.classifier = QueryClassifier()\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        category = self.classifier.classify(query)\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "205506d9-0c30-4557-9503-1549d1e2aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Local\\Temp\\ipykernel_15136\\4016186069.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class PydanticAdaptiveRetriever(BaseRetriever):\n",
      "C:\\Users\\Revathi\\AppData\\Local\\Temp\\ipykernel_15136\\4016186069.py:1: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class PydanticAdaptiveRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "class PydanticAdaptiveRetriever(BaseRetriever):\n",
    "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.adaptive_retriever.get_relevant_documents(query)\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a213ea1c-b65b-4cea-b088-879f0b1fceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        adaptive_retriever = AdaptiveRetriever(texts)\n",
    "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "        \n",
    "        # Create a custom prompt\n",
    "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        \n",
    "        # Create the LLM chain\n",
    "        self.llm_chain = prompt | self.llm\n",
    "        \n",
    "      \n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
    "        return self.llm_chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14450487-b9e4-4dff-8116-82a4f41395c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "    ]\n",
    "rag_system = AdaptiveRAG(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f37f038-bb62-4760-8c3b-d3834266ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifying query\n",
      "retrieving factual\n",
      "enhande query: What is the average distance between the Earth and the Sun in kilometers and miles, and how does this distance vary throughout the year due to the elliptical shape of Earth's orbit?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking docs\n",
      "Answer: I don't know.\n",
      "classifying query\n",
      "retrieving analytical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub queries for comprehensive analysis: ['What is the average distance of the Earth from the Sun, and how does it vary throughout the year?', \"How does the Earth's elliptical orbit influence seasonal climate variations?\", \"What role does the Earth's axial tilt play in conjunction with its distance from the Sun in affecting climate?\", \"How do changes in the Earth's distance from the Sun over geological time scales impact long-term climate patterns?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected diverse and relevant documents\n",
      "Answer: I don't know.\n",
      "classifying query\n",
      "retrieving analytical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub queries for comprehensive analysis: ['What is the primordial soup theory and how does it explain the origin of life on Earth?', 'How does the panspermia hypothesis propose life originated on Earth?', 'What role does the hydrothermal vent theory play in explaining the origin of life?', 'How do modern scientific experiments, like the Miller-Urey experiment, contribute to our understanding of the origin of life?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected diverse and relevant documents\n",
      "Answer: There are several theories about the origin of life on Earth, including:\n",
      "\n",
      "1. **Abiogenesis**: This theory suggests that life arose naturally from non-living matter through chemical processes. It posits that simple organic molecules gradually evolved into more complex forms, eventually leading to the first living organisms.\n",
      "\n",
      "2. **Panspermia**: This hypothesis proposes that life did not originate on Earth but was instead brought here from elsewhere in the universe, possibly via comets, meteorites, or cosmic dust.\n",
      "\n",
      "3. **Primordial Soup**: This concept is a variation of abiogenesis, suggesting that life began in a \"soup\" of organic molecules, possibly in the oceans, where energy sources like lightning or volcanic activity facilitated the formation of complex molecules.\n",
      "\n",
      "4. **Hydrothermal Vent Hypothesis**: This theory suggests that life may have originated at hydrothermal vents on the ocean floor, where mineral-laden water provides the necessary conditions for chemical reactions that could lead to life.\n",
      "\n",
      "5. **Clay Hypothesis**: Proposed by scientist Graham Cairns-Smith, this theory suggests that complex organic molecules might have formed on the surfaces of clay minerals, which could have acted as catalysts for chemical reactions.\n",
      "\n",
      "6. **RNA World Hypothesis**: This theory posits that self-replicating ribonucleic acid (RNA) molecules were precursors to current life forms, with RNA serving both as a catalyst and as a carrier of genetic information before the evolution of DNA and proteins.\n",
      "\n",
      "Each of these theories has its own supporting evidence and challenges, and the true origin of life on Earth remains a topic of scientific investigation and debate.\n",
      "classifying query\n",
      "retrieving analytical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub queries for comprehensive analysis: [\"What role does Earth's distance from the Sun play in maintaining temperatures suitable for life?\", \"How does Earth's position relative to other planets in the Solar System affect its gravitational stability and habitability?\", \"In what ways does Earth's orbit shape the seasonal variations that contribute to its habitability?\", \"How does the presence of the Moon, influenced by Earth's position in the Solar System, impact Earth's habitability?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected diverse and relevant documents\n",
      "Answer: The Earth's position as the third planet from the Sun places it in the \"habitable zone,\" also known as the \"Goldilocks zone,\" where conditions are just right for liquid water to exist on the surface. This is crucial for life as we know it because water is essential for most biological processes. Additionally, Earth's position allows it to have a stable climate and temperature range that supports diverse ecosystems. The distance from the Sun also ensures that Earth receives enough sunlight for photosynthesis, which is vital for plant life and, by extension, the entire food chain.\n"
     ]
    }
   ],
   "source": [
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content\n",
    "print(f\"Answer: {factual_result}\")\n",
    "\n",
    "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
    "print(f\"Answer: {analytical_result}\")\n",
    "\n",
    "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content\n",
    "print(f\"Answer: {opinion_result}\")\n",
    "\n",
    "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content\n",
    "print(f\"Answer: {contextual_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747242cc-0995-4a9e-bff0-854b6120bc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
