{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef4dd0d-f4c7-4267-9b6d-330054c33da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.docstore.document import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fbde8c-ec73-4148-bace-51eb9abba10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Local\\Temp\\ipykernel_27796\\4163251067.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from helper_functions import *\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\n",
    "from helper_functions import *\n",
    "from evaluation.rag_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b51aace-4107-4024-8508-5c2a0b3f3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f5d64c-b8a6-4668-bac3-d0e6f7bcc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define logging, llm and embeddings\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\",max_tokens=1000,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26716be-7857-4c67-97d8-52141c61a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def extract_text(item):\n",
    "    \"\"\"Extract text content from either a string or an AIMessage object.\"\"\"\n",
    "    if isinstance(item, AIMessage):\n",
    "        return item.content\n",
    "    return item\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Embed texts using OpenAIEmbeddings.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    logging.info(f\"Embedding {len(texts)} texts\")\n",
    "    return embeddings.embed_documents([extract_text(text) for text in texts])\n",
    "\n",
    "\n",
    "def perform_clustering(embeddings: np.ndarray, n_clusters: int = 10) -> np.ndarray:\n",
    "    \"\"\"Perform clustering on embeddings using Gaussian Mixture Model.\"\"\"\n",
    "    logging.info(f\"Performing clustering with {n_clusters} clusters\")\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    return gm.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "def summarize_texts(texts: List[str], llm: ChatOpenAI) -> str:\n",
    "    \"\"\"Summarize a list of texts using OpenAI.\"\"\"\n",
    "    logging.info(f\"Summarizing {len(texts)} texts\")\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely:\\n\\n{text}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    input_data = {\"text\": texts}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "\n",
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, level: int):\n",
    "    \"\"\"Visualize clusters using PCA.\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'Cluster Visualization - Level {level}')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7454ef9-4cd6-4d60-a267-ca25a39c4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_raptor_tree(texts: List[str], llm, max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n",
    "    results = {}\n",
    "    current_texts = [extract_text(text) for text in texts]\n",
    "    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n",
    "    \n",
    "    for level in range(1, max_levels + 1):\n",
    "        logging.info(f\"Processing level {level}\")\n",
    "        \n",
    "        embeddings = embed_texts(current_texts)\n",
    "        n_clusters = min(10, len(current_texts) // 2)\n",
    "        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'text': current_texts,\n",
    "            'embedding': embeddings,\n",
    "            'cluster': cluster_labels,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "        \n",
    "        results[level-1] = df\n",
    "        \n",
    "        summaries = []\n",
    "        new_metadata = []\n",
    "        for cluster in df['cluster'].unique():\n",
    "            cluster_docs = df[df['cluster'] == cluster]\n",
    "            cluster_texts = cluster_docs['text'].tolist()\n",
    "            cluster_metadata = cluster_docs['metadata'].tolist()\n",
    "            summary = summarize_texts(cluster_texts,llm)\n",
    "            summaries.append(summary)\n",
    "            new_metadata.append({\n",
    "                \"level\": level,\n",
    "                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n",
    "                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n",
    "                \"id\": f\"summary_{level}_{cluster}\"\n",
    "            })\n",
    "        current_texts = summaries\n",
    "        current_metadata = new_metadata\n",
    "        \n",
    "        if len(current_texts) <= 1:\n",
    "            results[level] = pd.DataFrame({\n",
    "                'text': current_texts,\n",
    "                'embedding': embed_texts(current_texts),\n",
    "                'cluster': [0],\n",
    "                'metadata': current_metadata\n",
    "            })\n",
    "            logging.info(f\"Stopping at level {level} as we have only one summary\")\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e191ed9-1867-4900-82bb-1d2e56c085e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(tree_results: Dict[int, pd.DataFrame], embeddings) -> FAISS:\n",
    "    \"\"\"Build a FAISS vectorstore from all texts in the RAPTOR tree.\"\"\"\n",
    "    all_texts = []\n",
    "    all_embeddings = []\n",
    "    all_metadatas = []\n",
    "\n",
    "    for level, df in tree_results.items():\n",
    "        all_texts.extend([str(text) for text in df['text'].tolist()])\n",
    "        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in\n",
    "                               df['embedding'].tolist()])\n",
    "        all_metadatas.extend(df['metadata'].tolist())\n",
    "\n",
    "    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n",
    "    documents = [Document(page_content=str(text), metadata=metadata)\n",
    "                 for text, metadata in zip(all_texts, all_metadatas)]\n",
    "    return FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6500e99-c89e-4740-9191-cb3af163181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_traversal_retrieval(query: str, vectorstore: FAISS, k: int = 3) -> List[Document]:\n",
    "    \"\"\"Perform tree traversal retrieval.\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n",
    "        if parent_ids:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n",
    "            )\n",
    "        else:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level\n",
    "            )\n",
    "        \n",
    "        if not docs or level == 0:\n",
    "            return docs\n",
    "        \n",
    "        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n",
    "        child_ids = [item for sublist in child_ids for item in sublist]  # Flatten the list\n",
    "        \n",
    "        child_docs = retrieve_level(level - 1, child_ids)\n",
    "        return docs + child_docs\n",
    "    \n",
    "    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n",
    "    return retrieve_level(max_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a9fde4d-722e-46d1-aa6f-9cdb09ba85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore: FAISS, llm: ChatOpenAI) -> ContextualCompressionRetriever:\n",
    "    \"\"\"Create a retriever with contextual compression.\"\"\"\n",
    "    logging.info(\"Creating contextual compression retriever\")\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "\n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35f62564-ece2-497a-b187-42fbd61924ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n",
    "    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    for level in range(max_level, -1, -1):\n",
    "        # Retrieve documents from the current level\n",
    "        level_docs = retriever.get_relevant_documents(\n",
    "            query,\n",
    "            filter=lambda meta: meta['level'] == level\n",
    "        )\n",
    "        all_retrieved_docs.extend(level_docs)\n",
    "        \n",
    "        # If we've found documents, retrieve their children from the next level down\n",
    "        if level_docs and level > 0:\n",
    "            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n",
    "            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n",
    "            \n",
    "            if child_ids:  # Only modify query if there are valid child IDs\n",
    "                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n",
    "                query += child_query\n",
    "    \n",
    "    return all_retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72bd4730-1367-4e36-b42a-2376a03bbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n",
    "    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval.\"\"\"\n",
    "    logging.info(f\"Processing query: {query}\")\n",
    "    \n",
    "    relevant_docs = hierarchical_retrieval(query, retriever, max_level)\n",
    "    \n",
    "    doc_details = []\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        doc_details.append({\n",
    "            \"index\": i,\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"level\": doc.metadata.get('level', 'Unknown'),\n",
    "            \"similarity_score\": doc.metadata.get('score', 'N/A')\n",
    "        })\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context, please answer the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    answer = chain.run(context=context, question=query)\n",
    "    \n",
    "    logging.info(\"Query processing completed\")\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": doc_details,\n",
    "        \"num_docs_retrieved\": len(relevant_docs),\n",
    "        \"context_used\": context,\n",
    "        \"answer\": answer,\n",
    "        \"model_used\": llm.model_name,\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def print_query_details(result: Dict[str, Any]):\n",
    "    \"\"\"Print detailed information about the query process, including tree level metadata.\"\"\"\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nNumber of documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"\\nRetrieved Documents:\")\n",
    "    for doc in result['retrieved_documents']:\n",
    "        print(f\"  Document {doc['index']}:\")\n",
    "        print(f\"    Content: {doc['content'][:100]}...\")  # Show first 100 characters\n",
    "        print(f\"    Similarity Score: {doc['similarity_score']}\")\n",
    "        print(f\"    Tree Level: {doc['metadata'].get('level', 'Unknown')}\")\n",
    "        print(f\"    Origin: {doc['metadata'].get('origin', 'Unknown')}\")\n",
    "        if 'child_docs' in doc['metadata']:\n",
    "            print(f\"    Number of Child Documents: {len(doc['metadata']['child_docs'])}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nContext used for answer generation:\")\n",
    "    print(result['context_used'])\n",
    "    \n",
    "    print(f\"\\nGenerated Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    print(f\"\\nModel Used: {result['model_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c98a4c2d-b906-4aef-9d13-1d535c4ff71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Revathi\\Documents\\GenAIProjects\\All_RAG_Techniques\\data\\Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5913e5e2-f09f-45e8-ae43-87c73600e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Texts\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "texts = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e42a84e5-6ec1-4b65-a5b6-511695a102c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 00:35:19,790 - INFO - Processing level 1\n",
      "2025-07-26 00:35:21,734 - INFO - Embedding 33 texts\n",
      "2025-07-26 00:35:22,644 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:35:23,639 - INFO - Performing clustering with 10 clusters\n",
      "2025-07-26 00:35:38,409 - INFO - Summarizing 3 texts\n",
      "2025-07-26 00:35:42,041 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:35:42,082 - INFO - Summarizing 2 texts\n",
      "2025-07-26 00:35:46,633 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:35:46,638 - INFO - Summarizing 3 texts\n",
      "2025-07-26 00:35:56,869 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:35:56,881 - INFO - Summarizing 7 texts\n",
      "2025-07-26 00:36:01,398 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:01,407 - INFO - Summarizing 2 texts\n",
      "2025-07-26 00:36:06,418 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:06,426 - INFO - Summarizing 4 texts\n",
      "2025-07-26 00:36:12,970 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:12,979 - INFO - Summarizing 2 texts\n",
      "2025-07-26 00:36:16,661 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:16,669 - INFO - Summarizing 1 texts\n",
      "2025-07-26 00:36:21,774 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:21,781 - INFO - Summarizing 7 texts\n",
      "2025-07-26 00:36:24,972 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:24,980 - INFO - Summarizing 2 texts\n",
      "2025-07-26 00:36:29,661 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:29,664 - INFO - Processing level 2\n",
      "2025-07-26 00:36:31,657 - INFO - Embedding 10 texts\n",
      "2025-07-26 00:36:33,040 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:33,465 - INFO - Performing clustering with 5 clusters\n",
      "2025-07-26 00:36:40,592 - INFO - Summarizing 1 texts\n",
      "2025-07-26 00:36:42,768 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:42,774 - INFO - Summarizing 1 texts\n",
      "2025-07-26 00:36:45,531 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:45,542 - INFO - Summarizing 4 texts\n",
      "2025-07-26 00:36:51,515 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:51,523 - INFO - Summarizing 3 texts\n",
      "2025-07-26 00:36:54,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:54,708 - INFO - Summarizing 1 texts\n",
      "2025-07-26 00:36:57,925 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:36:57,934 - INFO - Processing level 3\n",
      "2025-07-26 00:36:59,898 - INFO - Embedding 5 texts\n",
      "2025-07-26 00:37:00,581 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:37:01,005 - INFO - Performing clustering with 2 clusters\n",
      "2025-07-26 00:37:03,850 - INFO - Summarizing 3 texts\n",
      "2025-07-26 00:37:07,343 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:37:07,350 - INFO - Summarizing 2 texts\n",
      "2025-07-26 00:37:09,592 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Build the RAPTOR tree\n",
    "tree_results = build_raptor_tree(texts, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbddcc5-e1b4-49e1-93f8-705733256b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 00:41:39,559 - INFO - Building vectorstore with 48 texts\n",
      "2025-07-26 00:41:40,938 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:41:41,920 - INFO - Loading faiss with AVX512 support.\n",
      "2025-07-26 00:41:41,920 - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-07-26 00:41:41,920 - INFO - Loading faiss with AVX2 support.\n",
      "2025-07-26 00:41:41,986 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-07-26 00:41:42,003 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "# Build vectorstore\n",
    "vectorstore = build_vectorstore(tree_results,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f30cd1-98d1-4b23-b415-1ad154dacab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 00:41:57,497 - INFO - Creating contextual compression retriever\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = create_retriever(vectorstore,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b51e9882-ad30-492e-8c26-c30c2b6ab70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 00:42:05,784 - INFO - Processing query: What is the greenhouse effect?\n",
      "C:\\Users\\Revathi\\AppData\\Local\\Temp\\ipykernel_27796\\3697090061.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  level_docs = retriever.get_relevant_documents(\n",
      "2025-07-26 00:42:07,159 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:08,083 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:09,421 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:10,043 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:11,784 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:13,421 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:14,621 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:15,378 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:16,861 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:18,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:19,502 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:20,592 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:22,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\Revathi\\AppData\\Local\\Temp\\ipykernel_27796\\4180896722.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\Revathi\\AppData\\Local\\Temp\\ipykernel_27796\\4180896722.py:26: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = chain.run(context=context, question=query)\n",
      "2025-07-26 00:42:25,393 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-26 00:42:25,407 - INFO - Query processing completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the greenhouse effect?\n",
      "\n",
      "Number of documents retrieved: 9\n",
      "\n",
      "Retrieved Documents:\n",
      "  Document 1:\n",
      "    Content: The context does not provide a definition or explanation of the greenhouse effect. It primarily disc...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 2\n",
      "    Origin: summary_of_cluster_2_level_1\n",
      "\n",
      "  Document 2:\n",
      "    Content: The context provided does not specifically define the greenhouse effect. However, it mentions that r...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_6_level_0\n",
      "\n",
      "  Document 3:\n",
      "    Content: The provided context does not contain information specifically about the greenhouse effect. Therefor...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_1_level_0\n",
      "\n",
      "  Document 4:\n",
      "    Content: The provided context does not contain any information about the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_3_level_0\n",
      "\n",
      "  Document 5:\n",
      "    Content: The provided context does not contain any information about the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_5_level_0\n",
      "\n",
      "  Document 6:\n",
      "    Content: The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), met...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 7:\n",
      "    Content: The provided context does not contain information specifically about the greenhouse effect. Therefor...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 8:\n",
      "    Content: The provided context does not contain any information about the greenhouse effect. Therefore, there ...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 9:\n",
      "    Content: The provided context does not contain any information about the greenhouse effect. Therefore, there ...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "\n",
      "Context used for answer generation:\n",
      "The context does not provide a definition or explanation of the greenhouse effect. It primarily discusses climate change, its causes, and consequences.\n",
      "\n",
      "The context provided does not specifically define the greenhouse effect. However, it mentions that recent changes in climate are largely attributed to greenhouse gas emissions from fossil fuels, agriculture, and deforestation. The greenhouse effect generally refers to the process by which certain gases in the Earth's atmosphere trap heat, leading to an increase in global temperatures.\n",
      "\n",
      "The provided context does not contain information specifically about the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The provided context does not contain any information about the greenhouse effect.\n",
      "\n",
      "The provided context does not contain any information about the greenhouse effect.\n",
      "\n",
      "The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, keeping the planet warm enough to support life. However, human activities have intensified this process, leading to a warmer climate.\n",
      "\n",
      "The provided context does not contain information specifically about the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The provided context does not contain any information about the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The provided context does not contain any information about the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "Generated Answer:\n",
      "The greenhouse effect is a natural process in which certain gases in the Earth's atmosphere, known as greenhouse gases (such as carbon dioxide, methane, and nitrous oxide), trap heat from the sun. This process helps to keep the planet warm enough to support life. However, human activities, such as burning fossil fuels, agriculture, and deforestation, have increased the concentration of these gases, intensifying the greenhouse effect and contributing to global warming and climate change.\n",
      "\n",
      "Model Used: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "max_level = 3  # Adjust based on your tree depth\n",
    "query = \"What is the greenhouse effect?\"\n",
    "result = raptor_query(query, retriever, max_level)\n",
    "print_query_details(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c60e63-42c6-44e3-a378-9594feac7ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
