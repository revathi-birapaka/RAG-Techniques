{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b255ca-c30e-477d-a621-8307462c867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "sys.path.append(os.path.abspath(\n",
    "    os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "from helper_functions import *\n",
    "from evaluation.rag_evaluation import *\n",
    "\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c90738-d93b-4ec6-ba89-778c4b52f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Revathi\\Documents\\GenAIProjects\\All_RAG_Techniques\\data\\Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65384dc0-2ad0-4a44-824b-45b0ec7c0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c290563-aa87-4a15-b24a-73b4cb266a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ddd274-08e9-48b7-9d8f-33de68dce1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class RetrievalResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if retrieval is necessary\", description=\"Output only 'Yes' or 'No'.\")\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Given the query '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
    ")\n",
    "\n",
    "class RelevanceResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if context is relevant\", description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
    ")\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Generated response\", description=\"The generated response.\")\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
    ")\n",
    "\n",
    "class SupportResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if response is supported\", description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
    "support_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\", \"context\"],\n",
    "    template=\"Given the response '{response}' and the context '{context}', determine if the response is supported by the context. Output 'Fully supported', 'Partially supported', or 'No support'.\"\n",
    ")\n",
    "\n",
    "class UtilityResponse(BaseModel):\n",
    "    response: int = Field(..., title=\"Utility rating\", description=\"Rate the utility of the response from 1 to 5.\")\n",
    "utility_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"response\"],\n",
    "    template=\"Given the query '{query}' and the response '{response}', rate the utility of the response from 1 to 5.\"\n",
    ")\n",
    "\n",
    "# Create LLMChains for each step\n",
    "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
    "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
    "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
    "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
    "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65836b72-0856-44f7-a42d-976b03455bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_rag:\n",
    "    def __init__(self, path, top_k=3):\n",
    "        self.vectorstore = encode_pdf(path)\n",
    "        self.top_k = top_k\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)\n",
    "\n",
    "        # Create LLMChains for each step\n",
    "        self.retrieval_chain = retrieval_prompt | self.llm.with_structured_output(RetrievalResponse)\n",
    "        self.relevance_chain = relevance_prompt | self.llm.with_structured_output(RelevanceResponse)\n",
    "        self.generation_chain = generation_prompt | self.llm.with_structured_output(GenerationResponse)\n",
    "        self.support_chain = support_prompt | self.llm.with_structured_output(SupportResponse)\n",
    "        self.utility_chain = utility_prompt | self.llm.with_structured_output(UtilityResponse)\n",
    "\n",
    "    def run(self, query):\n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "        # Step 1: Determine if retrieval is necessary\n",
    "        print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "        input_data = {\"query\": query}\n",
    "        retrieval_decision = self.retrieval_chain.invoke(input_data).response.strip().lower()\n",
    "        print(f\"Retrieval decision: {retrieval_decision}\")\n",
    "\n",
    "        if retrieval_decision == 'yes':\n",
    "            # Step 2: Retrieve relevant documents\n",
    "            print(\"Step 2: Retrieving relevant documents...\")\n",
    "            docs = self.vectorstore.similarity_search(query, k=self.top_k)\n",
    "            contexts = [doc.page_content for doc in docs]\n",
    "            print(f\"Retrieved {len(contexts)} documents\")\n",
    "\n",
    "            # Step 3: Evaluate relevance of retrieved documents\n",
    "            print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
    "            relevant_contexts = []\n",
    "            for i, context in enumerate(contexts):\n",
    "                input_data = {\"query\": query, \"context\": context}\n",
    "                relevance = self.relevance_chain.invoke(input_data).response.strip().lower()\n",
    "                print(f\"Document {i + 1} relevance: {relevance}\")\n",
    "                if relevance == 'relevant':\n",
    "                    relevant_contexts.append(context)\n",
    "\n",
    "            print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
    "\n",
    "            # If no relevant contexts found, generate without retrieval\n",
    "            if not relevant_contexts:\n",
    "                print(\"No relevant contexts found. Generating without retrieval...\")\n",
    "                input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
    "                return self.generation_chain.invoke(input_data).response\n",
    "\n",
    "            # Step 4: Generate response using relevant contexts\n",
    "            print(\"Step 4: Generating responses using relevant contexts...\")\n",
    "            responses = []\n",
    "            for i, context in enumerate(relevant_contexts):\n",
    "                print(f\"Generating response for context {i + 1}...\")\n",
    "                input_data = {\"query\": query, \"context\": context}\n",
    "                response = self.generation_chain.invoke(input_data).response\n",
    "\n",
    "                # Step 5: Assess support\n",
    "                print(f\"Step 5: Assessing support for response {i + 1}...\")\n",
    "                input_data = {\"response\": response, \"context\": context}\n",
    "                support = self.support_chain.invoke(input_data).response.strip().lower()\n",
    "                print(f\"Support assessment: {support}\")\n",
    "\n",
    "                # Step 6: Evaluate utility\n",
    "                print(f\"Step 6: Evaluating utility for response {i + 1}...\")\n",
    "                input_data = {\"query\": query, \"response\": response}\n",
    "                utility = int(self.utility_chain.invoke(input_data).response)\n",
    "                print(f\"Utility score: {utility}\")\n",
    "\n",
    "                responses.append((response, support, utility))\n",
    "\n",
    "            # Select the best response based on support and utility\n",
    "            print(\"Selecting the best response...\")\n",
    "            best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
    "            print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
    "            return best_response[0]\n",
    "        else:\n",
    "            # Generate without retrieval\n",
    "            print(\"Generating without retrieval...\")\n",
    "            input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n",
    "            return self.generation_chain.invoke(input_data).response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca581c3-0ac6-4e1e-98a3-88211c01f78e",
   "metadata": {},
   "source": [
    "##### Generating Response for a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9f01a89-77f0-4145-82f1-ee9fde992383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is the impact of climate change on the environment?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: yes\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 3 documents\n",
      "Step 3: Evaluating relevance of retrieved documents...\n",
      "Document 1 relevance: relevant\n",
      "Document 2 relevance: relevant\n",
      "Document 3 relevance: relevant\n",
      "Number of relevant contexts: 3\n",
      "Step 4: Generating responses using relevant contexts...\n",
      "Generating response for context 1...\n",
      "Step 5: Assessing support for response 1...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 1...\n",
      "Utility score: 5\n",
      "Generating response for context 2...\n",
      "Step 5: Assessing support for response 2...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 2...\n",
      "Utility score: 5\n",
      "Generating response for context 3...\n",
      "Step 5: Assessing support for response 3...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 3...\n",
      "Utility score: 5\n",
      "Selecting the best response...\n",
      "Best response support: fully supported, utility: 5\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"C:\\Users\\Revathi\\Documents\\GenAIProjects\\All_RAG_Techniques\\data\\Understanding_Climate_Change.pdf\"  \n",
    "query = \"What is the impact of climate change on the environment?\"\n",
    "\n",
    "rag = self_rag(path=pdf_path)\n",
    "response = rag.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857af8a-8f60-4e3c-bec2-0b1b407dbbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3929a9fe-c87b-4b73-842b-bd47e18d9569",
   "metadata": {},
   "source": [
    "##### Generating Response with less relevant Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f771624e-1af2-4bf0-9d32-ff4f9d7d5076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1759: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: how did harry beat quirrell?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: yes\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 3 documents\n",
      "Step 3: Evaluating relevance of retrieved documents...\n",
      "Document 1 relevance: irrelevant\n",
      "Document 2 relevance: irrelevant\n",
      "Document 3 relevance: irrelevant\n",
      "Number of relevant contexts: 0\n",
      "No relevant contexts found. Generating without retrieval...\n"
     ]
    }
   ],
   "source": [
    "query = \"how did harry beat quirrell?\"\n",
    "rag = self_rag(path=pdf_path)\n",
    "response = rag.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0851ae5-c101-4397-989d-d718f3973677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
