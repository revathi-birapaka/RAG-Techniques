{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404ceefd-25f6-4655-9b40-2f83d4f0cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from scipy.special import logsumexp\n",
    "from typing import Tuple, List, Any\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "from helper_functions import *\n",
    "from evaluation.rag_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec98cb11-b0ba-4ae5-9099-13d1e4b00d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Revathi\\Documents\\GenAIProjects\\All_RAG_Techniques\\data\\Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf64a5d-1a7c-4a58-81a2-639868bfc677",
   "metadata": {},
   "source": [
    "##### Encoding the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0518580-7b7f-4248-88e4-a18af93d0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is same like simple_rag.ipynb, only simulating a dense dataset\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "    documents=documents*5 # load every document 5 times to emulate a dense dataset\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings (Tested with OpenAI and Amazon Bedrock)\n",
    "    embeddings = get_langchain_embedding_provider(EmbeddingProvider.OPENAI)\n",
    "    #embeddings = get_langchain_embedding_provider(EmbeddingProvider.AMAZON_BEDROCK)\n",
    "\n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc414f-586a-4033-9f0a-479f6ef07a5c",
   "metadata": {},
   "source": [
    "##### Creating the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896ff81e-922c-4bdb-8bc3-dce11fb1fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vector_store = encode_pdf(path, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3036a7ce-2503-4726-b520-5d07250be92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_text(idx:int):\n",
    "    \"\"\"\n",
    "    Convert a Vector store index to the corresponding text.\n",
    "    \"\"\"\n",
    "    docstore_id = chunks_vector_store.index_to_docstore_id[idx]\n",
    "    document = chunks_vector_store.docstore.search(docstore_id)\n",
    "    return document.page_content\n",
    "\n",
    "\n",
    "def get_context(query:str,k:int=5) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Retrieve top k context items for a query using top k retrieval.\n",
    "    \"\"\"\n",
    "    # regular top k retrieval\n",
    "    q_vec=chunks_vector_store.embedding_function.embed_documents([query])\n",
    "    _,indices=chunks_vector_store.index.search(np.array(q_vec),k=k)\n",
    "\n",
    "    texts = [idx_to_text(i) for i in indices[0]]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d24fe86-565d-47a8-9001-0e71e2a18d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What is the main cause of climate change?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e307515-8737-42bd-a75d-dd9e7de38511",
   "metadata": {},
   "source": [
    "###### Regular top k retrieval\n",
    "This demonstration shows that when database is dense (here we simulate density by loading each document 5 times), the results are not good, we don't get the most relevant results. Note that the top 3 results are all repetitions of the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb714553-6c95-4e4d-bb9d-eaaf71a7d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "\n",
      "Context 2:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "\n",
      "Context 3:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts=get_context(test_query,k=3)\n",
    "show_context(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423a027-8e92-452c-904f-d9881079ba67",
   "metadata": {},
   "source": [
    "##### More utils for distances normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1886dca1-5c3c-4fd4-b91f-4df5f9f509e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm(dist:np.ndarray, sigma:float):\n",
    "    \"\"\"\n",
    "    Calculate the log-normal probability for a given distance and sigma.\n",
    "    \"\"\"\n",
    "    if sigma < 1e-9: \n",
    "        return -np.inf * dist\n",
    "    return -np.log(sigma) - 0.5 * np.log(2 * np.pi) - dist**2 / (2 * sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291eb59f-d0d3-4911-8ffa-3d929a61b172",
   "metadata": {},
   "source": [
    "##### Greedy Dartboard Search\n",
    "This is the core algorithm: A search algorithm that selects a diverse set of relevant documents from a collection by balancing two factors: relevance to the query and diversity among selected documents.\n",
    "\n",
    "Given distances between a query and documents, plus distances between all documents, the algorithm:\n",
    "\n",
    "Selects the most relevant document first\n",
    "Iteratively selects additional documents by combining:\n",
    "Relevance to the original query\n",
    "Diversity from previously selected documents\n",
    "The balance between relevance and diversity is controlled by weights:\n",
    "\n",
    "DIVERSITY_WEIGHT: Importance of difference from existing selections\n",
    "RELEVANCE_WEIGHT: Importance of relevance to query\n",
    "SIGMA: Smoothing parameter for probability conversion\n",
    "The algorithm returns both the selected documents and their selection scores, making it useful for applications like search results where you want relevant but varied results.\n",
    "\n",
    "For example, when searching news articles, it would first return the most relevant article, then find articles that are both on-topic and provide new information, avoiding redundant selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98eb321f-a376-4c24-9184-270d90f76dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "DIVERSITY_WEIGHT = 1.0  # Weight for diversity in document selection\n",
    "RELEVANCE_WEIGHT = 1.0  # Weight for relevance to query\n",
    "SIGMA = 0.1  # Smoothing parameter for probability distribution\n",
    "\n",
    "def greedy_dartsearch(\n",
    "    query_distances: np.ndarray,\n",
    "    document_distances: np.ndarray,\n",
    "    documents: List[str],\n",
    "    num_results: int\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform greedy dartboard search to select top k documents balancing relevance and diversity.\n",
    "    \n",
    "    Args:\n",
    "        query_distances: Distance between query and each document\n",
    "        document_distances: Pairwise distances between documents\n",
    "        documents: List of document texts\n",
    "        num_results: Number of documents to return\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - List of selected document texts\n",
    "        - List of selection scores for each document\n",
    "    \"\"\"\n",
    "    # Avoid division by zero in probability calculations\n",
    "    sigma = max(SIGMA, 1e-5)\n",
    "    \n",
    "    # Convert distances to probability distributions\n",
    "    query_probabilities = lognorm(query_distances, sigma)\n",
    "    document_probabilities = lognorm(document_distances, sigma)\n",
    "    \n",
    "    # Initialize with most relevant document\n",
    "    \n",
    "    most_relevant_idx = np.argmax(query_probabilities)\n",
    "    selected_indices = np.array([most_relevant_idx])\n",
    "    selection_scores = [1.0] # dummy score for the first document\n",
    "    # Get initial distances from the first selected document\n",
    "    max_distances = document_probabilities[most_relevant_idx]\n",
    "    \n",
    "    # Select remaining documents\n",
    "    while len(selected_indices) < num_results:\n",
    "        # Update maximum distances considering new document\n",
    "        updated_distances = np.maximum(max_distances, document_probabilities)\n",
    "        \n",
    "        # Calculate combined diversity and relevance scores\n",
    "        combined_scores = (\n",
    "            updated_distances * DIVERSITY_WEIGHT +\n",
    "            query_probabilities * RELEVANCE_WEIGHT\n",
    "        )\n",
    "        \n",
    "        # Normalize scores and mask already selected documents\n",
    "        normalized_scores = logsumexp(combined_scores, axis=1)\n",
    "        normalized_scores[selected_indices] = -np.inf\n",
    "        \n",
    "        # Select best remaining document\n",
    "        best_idx = np.argmax(normalized_scores)\n",
    "        best_score = np.max(normalized_scores)\n",
    "        \n",
    "        # Update tracking variables\n",
    "        max_distances = updated_distances[best_idx]\n",
    "        selected_indices = np.append(selected_indices, best_idx)\n",
    "        selection_scores.append(best_score)\n",
    "    \n",
    "    # Return selected documents and their scores\n",
    "    selected_documents = [documents[i] for i in selected_indices]\n",
    "    return selected_documents, selection_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e1036-7039-46b4-9d03-f8107c26da76",
   "metadata": {},
   "source": [
    "##### Dartboard Context Retrieval\n",
    "Main function for using the dartboard retrieval. \n",
    "\n",
    "    This serves instead of get_context (which is simple RAG). \n",
    "    It:\n",
    "    1.Takes a text query, vectorizes it, gets the top k documents (and their vectors) via simple RAG\n",
    "    2.Uses these vectors to calculate the similarities to query and between candidate matches\n",
    "    3.Runs the dartboard algorithm to refine the candidate matches to a final list of k documents\n",
    "    4.Returns the final list of documents and their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "903e0579-99f6-44c8-87c7-c03489c188a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_with_dartboard(\n",
    "    query: str,\n",
    "    num_results: int = 3,\n",
    "    oversampling_factor: int = 3\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"\n",
    "    Retrieve most relevant and diverse context items for a query using the dartboard algorithm.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "        num_results: Number of context items to return (default: 5)\n",
    "        oversampling_factor: Factor to oversample initial results for better diversity (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - List of selected context texts\n",
    "        - List of selection scores\n",
    "        \n",
    "    Note:\n",
    "        The function uses cosine similarity converted to distance. Initial retrieval \n",
    "        fetches oversampling_factor * num_results items to ensure sufficient diversity \n",
    "        in the final selection.\n",
    "    \"\"\"\n",
    "    # Embed query and retrieve initial candidates\n",
    "    query_embedding = chunks_vector_store.embedding_function.embed_documents([query])\n",
    "    _, candidate_indices = chunks_vector_store.index.search(\n",
    "        np.array(query_embedding),\n",
    "        k=num_results * oversampling_factor\n",
    "    )\n",
    "    \n",
    "    # Get document vectors and texts for candidates\n",
    "    candidate_vectors = np.array(\n",
    "        chunks_vector_store.index.reconstruct_batch(candidate_indices[0])\n",
    "    )\n",
    "    candidate_texts = [idx_to_text(idx) for idx in candidate_indices[0]]\n",
    "    \n",
    "    # Calculate distance matrices\n",
    "    # Using 1 - cosine_similarity as distance metric\n",
    "    document_distances = 1 - np.dot(candidate_vectors, candidate_vectors.T)\n",
    "    query_distances = 1 - np.dot(query_embedding, candidate_vectors.T)\n",
    "    \n",
    "    # Apply dartboard selection algorithm\n",
    "    selected_texts, selection_scores = greedy_dartsearch(\n",
    "        query_distances,\n",
    "        document_distances,\n",
    "        candidate_texts,\n",
    "        num_results\n",
    "    )\n",
    "    \n",
    "    return selected_texts, selection_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38fe9bf8-d953-4275-9fb2-d4f19c0e0610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "\n",
      "Context 2:\n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies flourished, but the industrial era has seen \n",
      "unprecedented changes. \n",
      "Modern Observations \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhouse gases. \n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the\n",
      "\n",
      "\n",
      "Context 3:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts,scores=get_context_with_dartboard(test_query)\n",
    "show_context(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8003792-ba9f-439c-90d3-2e5a8dc2118c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
